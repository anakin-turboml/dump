{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## TurboML LLM Tutorial\n",
        "TurboML can spin up LLM servers with an OpenAI-compatible API. We currently support models\n",
        "in the GGUF format, but also support non-GGUF models that can be converted to GGUF. In the latter\n",
        "case you get to decide the quantization type you want to use."
      ],
      "metadata": {
        "id": "U2Bi92vcfIcx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set up the environment and install TurboML's SDK.\n",
        "We use `turboml-installer` to set up the environment for TurboML's SDK."
      ],
      "metadata": {
        "id": "q6ci1aQBfIcz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q turboml-installer\n",
        "import turboml_installer ; turboml_installer.install_on_colab()"
      ],
      "execution_count": 3,
      "metadata": {
        "id": "ER4GP-hkfIc0",
        "outputId": "0d11c67b-fe50-4703-a31d-e013aec0c947",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/1.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.6/1.6 MB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hüì¶ Installing...\n",
            "ü©π Patching environment...\n",
            "‚è≤ Done in 0:02:09\n",
            "üîÅ Restarting kernel...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The kernel should now be restarted with TurboML's SDK installed."
      ],
      "metadata": {
        "id": "_q-dvMAxfIc1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Login to your TurboML instance\n",
        "\n",
        "Note that you can copy and replace this snippet with one from your TurboML homepage."
      ],
      "metadata": {
        "id": "oCKnAQYMfIc1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import turboml as tb\n",
        "tb.init(\n",
        "  backend_url=\"http://34.93.248.93:8500\",\n",
        "  api_key=\"tb_Cn9wlAPvO09Qsak7rVfYqivOxdh9Tz88_17ef9683\"\n",
        ")"
      ],
      "execution_count": 2,
      "metadata": {
        "id": "sEj8EHHXfIc2",
        "outputId": "228a5f1a-f4da-442a-c3f1-0a1b68700e02",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'turboml'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-849eda7fee95>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mturboml\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m tb.init(\n\u001b[1;32m      3\u001b[0m   \u001b[0mbackend_url\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"http://34.93.248.93:8500\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mapi_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"tb_Cn9wlAPvO09Qsak7rVfYqivOxdh9Tz88_17ef9683\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m )\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'turboml'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "LlamaServerRequest = tb.llm.LlamaServerRequest\n",
        "HuggingFaceSpec = LlamaServerRequest.HuggingFaceSpec\n",
        "ServerParams = LlamaServerRequest.ServerParams"
      ],
      "execution_count": null,
      "metadata": {
        "id": "FAhyVxwqfIc3"
      },
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Choose a model\n",
        "Let's use a Llama 3.2 quant already in the GGUF format."
      ],
      "metadata": {
        "id": "mv-nRxRTfIc3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hf_spec = HuggingFaceSpec(\n",
        "    hf_repo_id=\"hugging-quants/Llama-3.2-1B-Instruct-Q4_K_M-GGUF\",\n",
        "    select_gguf_file=\"llama-3.2-1b-instruct-q4_k_m.gguf\",\n",
        ")"
      ],
      "execution_count": 1,
      "metadata": {
        "id": "1mOE_5SXfIc4",
        "outputId": "b019cbdf-aeb6-46a4-a539-b3c3f0294477",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'HuggingFaceSpec' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-2552b820d5e1>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m hf_spec = HuggingFaceSpec(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mhf_repo_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"hugging-quants/Llama-3.2-1B-Instruct-Q4_K_M-GGUF\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mselect_gguf_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"llama-3.2-1b-instruct-q4_k_m.gguf\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m )\n",
            "\u001b[0;31mNameError\u001b[0m: name 'HuggingFaceSpec' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Spawn a server\n",
        "On spawning a server, you get a `server_id` to reference it later as well as `server_relative_url` you can\n",
        "use to reach it. This method is synchronous, so it can take a while to yield as we retrieve (and convert) your model."
      ],
      "metadata": {
        "id": "phhcLT2WfIc5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = tb.llm.spawn_llm_server(\n",
        "    LlamaServerRequest(\n",
        "        source_type=LlamaServerRequest.SourceType.HUGGINGFACE,\n",
        "        hf_spec=hf_spec,\n",
        "        server_params=ServerParams(\n",
        "            threads=-1,\n",
        "            seed=-1,\n",
        "            context_size=0,\n",
        "            flash_attention=False,\n",
        "        ),\n",
        "    )\n",
        ")\n",
        "response"
      ],
      "execution_count": null,
      "metadata": {
        "id": "ABKE66jkfIc5"
      },
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "server_id = response.server_id"
      ],
      "execution_count": null,
      "metadata": {
        "id": "pfMlVRWyfIc5"
      },
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Interacting with the LLM\n",
        "\n",
        "Our LLM is exposed with an OpenAI-compatible API, so we can use the OpenAI SDK, or any\n",
        "other tool compatible tool to use it."
      ],
      "metadata": {
        "id": "YXsYBnlyfIc6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install openai"
      ],
      "execution_count": null,
      "metadata": {
        "id": "1y_b75BpfIc6"
      },
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "base_url = tb.common.env.CONFIG.TURBOML_BACKEND_SERVER_ADDRESS\n",
        "server_url = f\"{base_url}/{response.server_relative_url}\"\n",
        "\n",
        "client = OpenAI(base_url=server_url, api_key=\"-\")\n",
        "\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Hello there how are you doing today?\",\n",
        "        }\n",
        "    ],\n",
        "    model=\"-\",\n",
        ")\n",
        "\n",
        "print(response)"
      ],
      "execution_count": null,
      "metadata": {
        "id": "J19phAKafIc6"
      },
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = (\n",
        "    client.embeddings.create(input=[\"Hello there how are you doing today?\"], model=\"-\")\n",
        "    .data[0]\n",
        "    .embedding\n",
        ")\n",
        "len(embeddings), embeddings[:5]"
      ],
      "execution_count": null,
      "metadata": {
        "id": "KbHi5VQXfIc6"
      },
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stop the server"
      ],
      "metadata": {
        "id": "PonTAE04fIc7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tb.llm.stop_llm_server(server_id)"
      ],
      "execution_count": null,
      "metadata": {
        "id": "QmKtpbbyfIc8"
      },
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}